{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjHp7SysJf-f",
        "outputId": "d146f94e-3f9c-4da4-ad2e-44c8d4370dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/isic16\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sarlren/isic16\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IysWIYJdLvzA",
        "outputId": "70728dc7-46c3-438f-bcfb-a5f867ab2cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedSegDiff'...\n",
            "remote: Enumerating objects: 716, done.\u001b[K\n",
            "remote: Counting objects: 100% (397/397), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 716 (delta 340), reused 248 (delta 248), pack-reused 319 (from 2)\u001b[K\n",
            "Receiving objects: 100% (716/716), 3.90 MiB | 12.94 MiB/s, done.\n",
            "Resolving deltas: 100% (396/396), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WuJunde/MedSegDiff.git\n",
        "!mv MedSegDiff/* ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bugged section: x=x[:,-1:,...]  #loss is only calculated on the last channel, not on the input brain MR image\n",
        "# becomes:\n",
        "# x=x[:,-1:,...]\\n        if x.shape != model_output.shape:\\n            model_output = model_output[:,0,...]\n",
        "!sed -i 's/.*x=x[:,-1:,...].*/        x=x[:,-1:,...]\\n        if x.shape != model_output.shape:\\n            model_output = model_output[:,[0],...]/' /content/guided_diffusion/gaussian_diffusion.py\n",
        "!sed -n '275,+20p'  /content/guided_diffusion/gaussian_diffusion.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYP4thQexUSJ",
        "outputId": "5be83c05-114f-4cec-aa12-42e522f50571"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        x=x[:,-1:,...]\n",
            "        if x.shape != model_output.shape:\n",
            "            model_output = model_output[:,[0],...]\n",
            "        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n",
            "            assert model_output.shape == (B, C * 2, *x.shape[2:])\n",
            "            model_output, model_var_values = th.split(model_output, C, dim=1)\n",
            "            if self.model_var_type == ModelVarType.LEARNED:\n",
            "                model_log_variance = model_var_values\n",
            "                model_variance = th.exp(model_log_variance)\n",
            "            else:\n",
            "                min_log = _extract_into_tensor(\n",
            "                    self.posterior_log_variance_clipped, t, x.shape\n",
            "                )\n",
            "                max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n",
            "                # The model_var_values is [-1, 1] for [min_var, max_var].\n",
            "                frac = (model_var_values + 1) / 2\n",
            "                model_log_variance = frac * max_log + (1 - frac) * min_log\n",
            "                model_variance = th.exp(model_log_variance)\n",
            "        else:\n",
            "            model_variance, model_log_variance = {\n",
            "                # for fixedlarge, we set the initial (log-)variance like so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExdmMQJmKbpQ",
        "outputId": "596f4398-72cd-4aee-d97e-2f3e15df16af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kqDGAVprODFj"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wPgkhLO1ODi0"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fj_l69maOFoc"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working/isic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mmB8pgj5cwBe"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/guided_diffusion /content/scripts/guided_diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g2Qy0qHqODrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d96efc-f4dc-4e8d-83fc-6122f203c21b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import subprocess\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Test_Data/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Training_Data/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Training_GroundTruth/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Test_GroundTruth/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp \"+path+\"/ISBI2016_ISIC_Part3B_Test_GroundTruth.csv /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp \"+path+\"/ISBI2016_ISIC_Part3B_Training_GroundTruth.csv /kaggle/working/isic/\", shell=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "X3KfrcTjJ8VL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df6115a4-ff69-4c3a-e74d-aa035e3a3ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting blobfile\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile) (5.4.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile) (3.18.0)\n",
            "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodomex, blobfile\n",
            "Successfully installed blobfile-3.0.0 pycryptodomex-3.22.0\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.13.2)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from visdom) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom) (1.15.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from visdom) (3.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from visdom) (11.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2025.4.26)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=c86de94669e4a99b92847a4c9538560247e4f6315cb3a49bc943c5669027d8c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built visdom\n",
            "Installing collected packages: visdom\n",
            "Successfully installed visdom-0.2.4\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Collecting batchgenerators\n",
            "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (11.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (1.15.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (1.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (2.2.2)\n",
            "Collecting unittest2 (from batchgenerators)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->batchgenerators) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->batchgenerators) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->batchgenerators) (2025.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->batchgenerators) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->batchgenerators) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->batchgenerators) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->batchgenerators) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->batchgenerators) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->batchgenerators) (1.4.2)\n",
            "Collecting argparse (from unittest2->batchgenerators)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.11/dist-packages (from unittest2->batchgenerators) (1.17.0)\n",
            "Collecting traceback2 (from unittest2->batchgenerators)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: batchgenerators\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=966cee3a16b73e014c5508f1fa3b0e75630c990d69354a42463ee91d904f9a25\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/11/c7/fadca30e054c602093ffe36ba8a2f0a87dd2f86ac75191d3ed\n",
            "Successfully built batchgenerators\n",
            "Installing collected packages: linecache2, argparse, traceback2, unittest2, batchgenerators\n",
            "Successfully installed argparse-1.4.0 batchgenerators-0.25.1 linecache2-1.0.0 traceback2-1.4.0 unittest2-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "8fa7f8ad6444423599651ee3594ffb1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4458271 sha256=eb87a717f843039bb6dd05f4ee2bfd696a64e10135c6ccac5d773720651ef997\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install blobfile\n",
        "!pip install nibabel\n",
        "!pip install visdom\n",
        "!pip install torchsummary\n",
        "!pip install batchgenerators\n",
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb"
      ],
      "metadata": {
        "id": "eep4KKwVuOyi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2Z6pr6GhgFOG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/scripts\")\n",
        "sys.path.append(\"./\")\n",
        "from ssl import OP_NO_TLSv1\n",
        "import nibabel as nib\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image\n",
        "import torch.distributed as dist\n",
        "import torchvision.utils as vutils\n",
        "from guided_diffusion import dist_util, logger\n",
        "from guided_diffusion.resample import create_named_schedule_sampler\n",
        "from guided_diffusion.bratsloader import BRATSDataset, BRATSDataset3D\n",
        "from guided_diffusion.isicloader import ISICDataset\n",
        "from guided_diffusion.custom_dataset_loader import CustomDataset\n",
        "from guided_diffusion.script_util import (\n",
        "    NUM_CLASSES,\n",
        "    model_and_diffusion_defaults,\n",
        "    create_model_and_diffusion,\n",
        "    args_to_dict,\n",
        "    add_dict_to_argparser,\n",
        ")\n",
        "import torch as th\n",
        "from guided_diffusion.train_util import TrainLoop\n",
        "from guided_diffusion.utils import staple\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "  data_name: str\n",
        "  data_dir: str\n",
        "  out_dir: str\n",
        "  image_size: int\n",
        "  num_channels: int\n",
        "  class_cond: bool\n",
        "  num_res_blocks: int\n",
        "  num_heads: int\n",
        "  learn_sigma: bool\n",
        "  use_scale_shift_norm: bool\n",
        "  attention_resolutions: str\n",
        "  diffusion_steps: int\n",
        "  noise_schedule: str\n",
        "  rescale_learned_sigmas: bool\n",
        "  rescale_timesteps: bool\n",
        "  lr: float\n",
        "  batch_size: int\n",
        "  microbatch: int\n",
        "  ema_rate: str\n",
        "  log_interval: int\n",
        "  save_interval: int\n",
        "  resume_checkpoint: str\n",
        "  schedule_sampler: str\n",
        "  weight_decay: float\n",
        "  lr_anneal_steps: int\n",
        "  use_fp16: bool\n",
        "  fp16_scale_growth: float\n",
        "  gpu_dev: str\n",
        "  multi_gpu: str\n",
        "  in_ch: int\n",
        "  num_heads_upsample: int\n",
        "  num_head_channels: int\n",
        "  resblock_updown: bool\n",
        "  dpm_solver: bool\n",
        "  version: str\n",
        "  channel_mult: str\n",
        "  dropout: float\n",
        "  use_checkpoint: bool\n",
        "  use_new_attention_order: bool\n",
        "  timestep_respacing: str\n",
        "  use_kl: bool\n",
        "  predict_xstart: bool\n",
        "  model_path: str\n",
        "  num_ensemble: int\n",
        "  use_ddim: bool\n",
        "  clip_denoised: bool\n",
        "  debug: bool\n",
        "\n",
        "argsdict = dict(\n",
        "  data_name = 'ISIC',\n",
        "  data_dir = \"/kaggle/working/isic\",\n",
        "  out_dir = \"/kaggle/working/out\",\n",
        "  image_size = 256,\n",
        "  num_channels = 128,\n",
        "  class_cond = False,\n",
        "  num_res_blocks = 2,\n",
        "  num_heads = 1,\n",
        "  learn_sigma = True,\n",
        "  use_scale_shift_norm = False,\n",
        "  attention_resolutions = \"16\",\n",
        "  diffusion_steps = 1000,\n",
        "  noise_schedule = 'linear',\n",
        "  rescale_learned_sigmas = False,\n",
        "  rescale_timesteps = False,\n",
        "  lr = 1e-4,\n",
        "  batch_size = 4,\n",
        "  microbatch = -1,  # -1 disables microbatches\n",
        "  ema_rate = \"0.9999\",  # comma-separated list of EMA values\n",
        "  log_interval = 100,\n",
        "  save_interval = 5000,\n",
        "  resume_checkpoint = None, #\"/results/pretrainedmodel.pt\"\n",
        "  schedule_sampler=\"uniform\",\n",
        "  weight_decay=0.0,\n",
        "  lr_anneal_steps=400, # used for early stopping\n",
        "  use_fp16=False,\n",
        "  fp16_scale_growth=1e-3,\n",
        "  gpu_dev = \"0\",\n",
        "  multi_gpu = None, #\"0,1,2\"\n",
        "\n",
        "  # useful for sampling only:\n",
        "  model_path = \"/kaggle/working/out/savedmodel000400.pt\",\n",
        "  num_ensemble = 5,\n",
        "  use_ddim = False, #originally False\n",
        "  clip_denoised = True,\n",
        "  debug = False, # can always change\n",
        ")\n",
        "argsdict.update(model_and_diffusion_defaults())\n",
        "\n",
        "args = Args(**argsdict)\n",
        "\n",
        "seed=10\n",
        "th.manual_seed(seed)\n",
        "th.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    dist_util.setup_dist(args)\n",
        "    logger.configure(dir = args.out_dir)\n",
        "\n",
        "    logger.log(\"creating data loader...\")\n",
        "\n",
        "    if args.data_name == 'ISIC':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = ISICDataset(args, args.data_dir, transform_train)\n",
        "        args.in_ch = 4\n",
        "    elif args.data_name == 'BRATS':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = BRATSDataset3D(args.data_dir, transform_train, test_flag=False)\n",
        "        args.in_ch = 5\n",
        "    else :\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "        print(\"Your current directory : \",args.data_dir)\n",
        "        ds = CustomDataset(args, args.data_dir, transform_train)\n",
        "        args.in_ch = 4\n",
        "\n",
        "    datal= th.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True)\n",
        "    data = iter(datal)\n",
        "\n",
        "    logger.log(\"creating model and diffusion...\")\n",
        "\n",
        "    model, diffusion = create_model_and_diffusion(\n",
        "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
        "    )\n",
        "    if args.multi_gpu:\n",
        "        model = th.nn.DataParallel(model,device_ids=[int(id) for id in args.multi_gpu.split(',')])\n",
        "        model.to(device = th.device('cuda', int(args.gpu_dev)))\n",
        "    else:\n",
        "        model.to(dist_util.dev())\n",
        "    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion,  maxt=args.diffusion_steps)\n",
        "\n",
        "\n",
        "    logger.log(\"training...\")\n",
        "    TrainLoop(\n",
        "        model=model,\n",
        "        diffusion=diffusion,\n",
        "        classifier=None,\n",
        "        data=data,\n",
        "        dataloader=datal,\n",
        "        batch_size=args.batch_size,\n",
        "        microbatch=args.microbatch,\n",
        "        lr=args.lr,\n",
        "        ema_rate=args.ema_rate,\n",
        "        log_interval=args.log_interval,\n",
        "        save_interval=args.save_interval,\n",
        "        resume_checkpoint=args.resume_checkpoint,\n",
        "        use_fp16=args.use_fp16,\n",
        "        fp16_scale_growth=args.fp16_scale_growth,\n",
        "        schedule_sampler=schedule_sampler,\n",
        "        weight_decay=args.weight_decay,\n",
        "        lr_anneal_steps=args.lr_anneal_steps,\n",
        "    ).run_loop()\n",
        "\n",
        "def visualize(img):\n",
        "    _min = img.min()\n",
        "    _max = img.max()\n",
        "    normalized_img = (img - _min)/ (_max - _min)\n",
        "    return normalized_img\n",
        "\n",
        "\n",
        "def sample():\n",
        "    dist_util.setup_dist(args)\n",
        "    logger.configure(dir = args.out_dir)\n",
        "\n",
        "    if args.data_name == 'ISIC':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = ISICDataset(args, args.data_dir, transform_test, mode = 'Test')\n",
        "        args.in_ch = 4\n",
        "    elif args.data_name == 'BRATS':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)),]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = BRATSDataset3D(args.data_dir,transform_test)\n",
        "        args.in_ch = 5\n",
        "    else:\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor()]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = CustomDataset(args, args.data_dir, transform_test, mode = 'Test')\n",
        "        args.in_ch = 4\n",
        "\n",
        "    datal = th.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True)\n",
        "    data = iter(datal)\n",
        "\n",
        "    logger.log(\"creating model and diffusion...\")\n",
        "\n",
        "    model, diffusion = create_model_and_diffusion(\n",
        "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
        "    )\n",
        "    all_images = []\n",
        "\n",
        "\n",
        "    state_dict = dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n",
        "    from collections import OrderedDict\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        # name = k[7:] # remove `module.`\n",
        "        if 'module.' in k:\n",
        "            new_state_dict[k[7:]] = v\n",
        "            # load params\n",
        "        else:\n",
        "            new_state_dict = state_dict\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "    model.to(dist_util.dev())\n",
        "    if args.use_fp16:\n",
        "        model.convert_to_fp16()\n",
        "    model.eval()\n",
        "    for _ in range(len(data)):\n",
        "        b, m, path = next(data)  #should return an image from the dataloader \"data\"\n",
        "        c = th.randn_like(b[:, :1, ...])\n",
        "        img = th.cat((b, c), dim=1)     #add a noise channel$\n",
        "        if args.data_name == 'ISIC':\n",
        "            slice_ID=path[0].split(\"_\")[-1].split('.')[0]\n",
        "        elif args.data_name == 'BRATS':\n",
        "            # slice_ID=path[0].split(\"_\")[2] + \"_\" + path[0].split(\"_\")[4]\n",
        "            slice_ID=path[0].split(\"_\")[-3] + \"_\" + path[0].split(\"slice\")[-1].split('.nii')[0]\n",
        "\n",
        "        logger.log(\"sampling...\")\n",
        "\n",
        "        start = th.cuda.Event(enable_timing=True)\n",
        "        end = th.cuda.Event(enable_timing=True)\n",
        "        enslist = []\n",
        "\n",
        "        for i in range(args.num_ensemble):  #this is for the generation of an ensemble of 5 masks.\n",
        "            model_kwargs = {}\n",
        "            start.record()\n",
        "            sample_fn = (\n",
        "                diffusion.p_sample_loop_known if not args.use_ddim else diffusion.ddim_sample_loop_known\n",
        "            )\n",
        "            # pdb.set_trace() #imbedded debug !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "            # truncating model output after its internally calculated in dim 2 should fix the issue:\n",
        "            # https://github.com/SuperMedIntel/MedSegDiff/issues/201\n",
        "            # fixable by just editing gaussian_diffusion.py:346 to be permissive\n",
        "            sample, x_noisy, org, cal, cal_out = sample_fn( # broken\n",
        "                model,\n",
        "                (args.batch_size, 3, args.image_size, args.image_size), img,\n",
        "                step = args.diffusion_steps,\n",
        "                clip_denoised=args.clip_denoised,\n",
        "                model_kwargs=model_kwargs,\n",
        "            )\n",
        "\n",
        "            end.record()\n",
        "            th.cuda.synchronize()\n",
        "            print('time for 1 sample', start.elapsed_time(end))  #time measurement for the generation of 1 sample\n",
        "\n",
        "            co = th.tensor(cal_out)\n",
        "            if args.version == 'new':\n",
        "                enslist.append(sample[:,-1,:,:])\n",
        "            else:\n",
        "                enslist.append(co)\n",
        "\n",
        "            if args.debug:\n",
        "                # print('sample size is',sample.size())\n",
        "                # print('org size is',org.size())\n",
        "                # print('cal size is',cal.size())\n",
        "                if args.data_name == 'ISIC':\n",
        "                    # s = th.tensor(sample)[:,-1,:,:].unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "                    o = th.tensor(org)[:,:-1,:,:]\n",
        "                    c = th.tensor(cal).repeat(1, 3, 1, 1)\n",
        "                    # co = co.repeat(1, 3, 1, 1)\n",
        "\n",
        "                    s = sample[:,-1,:,:]\n",
        "                    b,h,w = s.size()\n",
        "                    ss = s.clone()\n",
        "                    ss = ss.view(s.size(0), -1)\n",
        "                    ss -= ss.min(1, keepdim=True)[0]\n",
        "                    ss /= ss.max(1, keepdim=True)[0]\n",
        "                    ss = ss.view(b, h, w)\n",
        "                    ss = ss.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "                    tup = (ss,o,c)\n",
        "                elif args.data_name == 'BRATS':\n",
        "                    s = th.tensor(sample)[:,-1,:,:].unsqueeze(1)\n",
        "                    m = th.tensor(m.to(device = 'cuda:0'))[:,0,:,:].unsqueeze(1)\n",
        "                    o1 = th.tensor(org)[:,0,:,:].unsqueeze(1)\n",
        "                    o2 = th.tensor(org)[:,1,:,:].unsqueeze(1)\n",
        "                    o3 = th.tensor(org)[:,2,:,:].unsqueeze(1)\n",
        "                    o4 = th.tensor(org)[:,3,:,:].unsqueeze(1)\n",
        "                    c = th.tensor(cal)\n",
        "\n",
        "                    tup = (o1/o1.max(),o2/o2.max(),o3/o3.max(),o4/o4.max(),m,s,c,co)\n",
        "\n",
        "                compose = th.cat(tup,0)\n",
        "                vutils.save_image(compose, fp = os.path.join(args.out_dir, str(slice_ID)+'_output'+str(i)+\".jpg\"), nrow = 1, padding = 10)\n",
        "        ensres = staple(th.stack(enslist,dim=0)).squeeze(0)\n",
        "        vutils.save_image(ensres, fp = os.path.join(args.out_dir, str(slice_ID)+'_output_ens'+\".jpg\"), nrow = 1, padding = 10)"
      ],
      "metadata": {
        "id": "9wuvHH-DiCRk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! rm -r /content/scripts/guided_diffusion/"
      ],
      "metadata": {
        "id": "J-aF2aixujhp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "IGQ1YgmHi3Vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30425543-79ef-41b1-a477-fc8d7573fe9b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /kaggle/working/out\n",
            "creating data loader...\n",
            "creating model and diffusion...\n",
            "training...\n",
            "---------------------------\n",
            "| grad_norm    | 21.9     |\n",
            "| loss         | 1        |\n",
            "| loss_cal     | 0.386    |\n",
            "| loss_cal_q0  | 0.482    |\n",
            "| loss_cal_q2  | 0.374    |\n",
            "| loss_cal_q3  | 0.315    |\n",
            "| loss_diff    | 1        |\n",
            "| loss_diff_q0 | 1.01     |\n",
            "| loss_diff_q2 | 0.995    |\n",
            "| loss_diff_q3 | 1.01     |\n",
            "| loss_q0      | 1.01     |\n",
            "| loss_q2      | 0.995    |\n",
            "| loss_q3      | 1.01     |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 4        |\n",
            "| step         | 0        |\n",
            "---------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n",
            "---------------------------\n",
            "| grad_norm    | 16.1     |\n",
            "| loss         | 0.258    |\n",
            "| loss_cal     | 0.236    |\n",
            "| loss_cal_q0  | 0.221    |\n",
            "| loss_cal_q1  | 0.242    |\n",
            "| loss_cal_q2  | 0.245    |\n",
            "| loss_cal_q3  | 0.238    |\n",
            "| loss_diff    | 0.258    |\n",
            "| loss_diff_q0 | 0.335    |\n",
            "| loss_diff_q1 | 0.256    |\n",
            "| loss_diff_q2 | 0.228    |\n",
            "| loss_diff_q3 | 0.21     |\n",
            "| loss_q0      | 0.335    |\n",
            "| loss_q1      | 0.256    |\n",
            "| loss_q2      | 0.228    |\n",
            "| loss_q3      | 0.21     |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 404      |\n",
            "| step         | 100      |\n",
            "---------------------------\n",
            "---------------------------\n",
            "| grad_norm    | 10.7     |\n",
            "| loss         | 0.0463   |\n",
            "| loss_cal     | 0.159    |\n",
            "| loss_cal_q0  | 0.165    |\n",
            "| loss_cal_q1  | 0.151    |\n",
            "| loss_cal_q2  | 0.153    |\n",
            "| loss_cal_q3  | 0.165    |\n",
            "| loss_diff    | 0.0463   |\n",
            "| loss_diff_q0 | 0.124    |\n",
            "| loss_diff_q1 | 0.0291   |\n",
            "| loss_diff_q2 | 0.0162   |\n",
            "| loss_diff_q3 | 0.0158   |\n",
            "| loss_q0      | 0.124    |\n",
            "| loss_q1      | 0.0291   |\n",
            "| loss_q2      | 0.0162   |\n",
            "| loss_q3      | 0.0158   |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 804      |\n",
            "| step         | 200      |\n",
            "---------------------------\n",
            "---------------------------\n",
            "| grad_norm    | 10.4     |\n",
            "| loss         | 0.0274   |\n",
            "| loss_cal     | 0.15     |\n",
            "| loss_cal_q0  | 0.158    |\n",
            "| loss_cal_q1  | 0.149    |\n",
            "| loss_cal_q2  | 0.142    |\n",
            "| loss_cal_q3  | 0.147    |\n",
            "| loss_diff    | 0.0274   |\n",
            "| loss_diff_q0 | 0.0719   |\n",
            "| loss_diff_q1 | 0.0163   |\n",
            "| loss_diff_q2 | 0.00697  |\n",
            "| loss_diff_q3 | 0.00616  |\n",
            "| loss_q0      | 0.0719   |\n",
            "| loss_q1      | 0.0163   |\n",
            "| loss_q2      | 0.00697  |\n",
            "| loss_q3      | 0.00616  |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 1.2e+03  |\n",
            "| step         | 300      |\n",
            "---------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y5lkM8JvL9fM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba77e31-fa47-4a83-96f4-ebb8ac0b2762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emasavedmodel_0.9999_000000.pt\toptsavedmodel000000.pt\tsavedmodel000000.pt\n",
            "emasavedmodel_0.9999_000400.pt\toptsavedmodel000400.pt\tsavedmodel000400.pt\n",
            "log.txt\t\t\t\tprogress.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "udrPkrJqL9_k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0573ef0c-ad40-4f94-ea88-15003e55d3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/working/out\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/kaggle/working/out/optsavedmodel000000.pt"
            ],
            "text/html": [
              "<a href='/kaggle/working/out/optsavedmodel000000.pt' target='_blank'>/kaggle/working/out/optsavedmodel000000.pt</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%cd /kaggle/working/out\n",
        "from IPython.display import FileLink\n",
        "FileLink('/kaggle/working/out/optsavedmodel000000.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B_3tkd7lMAMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a315d10d-3a75-49ab-8059-033f7f98bd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "tar: /kaggle/working/out/models.tar: file is the archive; not dumped\n"
          ]
        }
      ],
      "source": [
        "!tar -cf models.tar /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MLMGVTlMMBrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536a3ee3-280d-4cde-9ecc-6480fb9649e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emasavedmodel_0.9999_000000.pt\tmodels.tar\t\tprogress.csv\n",
            "emasavedmodel_0.9999_000400.pt\toptsavedmodel000000.pt\tsavedmodel000000.pt\n",
            "log.txt\t\t\t\toptsavedmodel000400.pt\tsavedmodel000400.pt\n"
          ]
        }
      ],
      "source": [
        "!ls /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7pHdtluDMC5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85042b0-44af-432e-9244-628f5aef1d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/kaggle/working/out/\n",
            "/kaggle/working/out/savedmodel000400.pt\n",
            "/kaggle/working/out/savedmodel000000.pt\n",
            "/kaggle/working/out/optsavedmodel000400.pt\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000000.pt\n",
            "/kaggle/working/out/model_tar2.tar.gz\n",
            "tar: /kaggle/working/out/model_tar2.tar.gz: file changed as we read it\n",
            "/kaggle/working/out/optsavedmodel000000.pt\n",
            "/kaggle/working/out/progress.csv\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000400.pt\n",
            "/kaggle/working/out/log.txt\n",
            "/kaggle/working/out/models.tar\n",
            "tar: /kaggle/working/out: file changed as we read it\n"
          ]
        }
      ],
      "source": [
        "!tar czvf model_tar2.tar.gz /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1NgQslhBMElC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af89efe-979d-4463-9699-9ba28689b2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 13G\n",
            "drwxr-xr-x 2 root root 4.0K May  3 20:57 .\n",
            "drwxr-xr-x 4 root root 4.0K May  3 20:49 ..\n",
            "-rw-r--r-- 1 root root 412M May  3 20:54 emasavedmodel_0.9999_000000.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 20:57 emasavedmodel_0.9999_000400.pt\n",
            "-rw-r--r-- 1 root root 2.4K May  3 20:57 log.txt\n",
            "-rw-r--r-- 1 root root 3.3G May  3 20:57 models.tar\n",
            "-rw-r--r-- 1 root root 5.9G May  3 21:04 model_tar2.tar.gz\n",
            "-rw-r--r-- 1 root root 823M May  3 20:54 optsavedmodel000000.pt\n",
            "-rw-r--r-- 1 root root 823M May  3 20:57 optsavedmodel000400.pt\n",
            "-rw-r--r-- 1 root root 1.1K May  3 20:56 progress.csv\n",
            "-rw-r--r-- 1 root root 412M May  3 20:54 savedmodel000000.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 20:57 savedmodel000400.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -alh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "b6h7nl_dMF3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8cd867-270a-4a85-addd-e194edf04b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drwxr-xr-x root/root         0 2025-05-03 20:57 kaggle/working/out/\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 20:57 kaggle/working/out/savedmodel000400.pt\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 20:54 kaggle/working/out/savedmodel000000.pt\n",
            "-rw-r--r-- root/root 862379209 2025-05-03 20:57 kaggle/working/out/optsavedmodel000400.pt\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 20:54 kaggle/working/out/emasavedmodel_0.9999_000000.pt\n",
            "-rw-r--r-- root/root 1769996288 2025-05-03 20:59 kaggle/working/out/model_tar2.tar.gz\n",
            "-rw-r--r-- root/root  862379209 2025-05-03 20:54 kaggle/working/out/optsavedmodel000000.pt\n",
            "-rw-r--r-- root/root       1095 2025-05-03 20:56 kaggle/working/out/progress.csv\n",
            "-rw-r--r-- root/root  431251992 2025-05-03 20:57 kaggle/working/out/emasavedmodel_0.9999_000400.pt\n",
            "-rw-r--r-- root/root       2449 2025-05-03 20:57 kaggle/working/out/log.txt\n",
            "-rw-r--r-- root/root 3449784320 2025-05-03 20:57 kaggle/working/out/models.tar\n"
          ]
        }
      ],
      "source": [
        "!tar -tvf ./model_tar2.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "f65s16gQguvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291d6005-e89f-41d7-cafb-c978d07060c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia_smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia_smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZDlerMDMQP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64cdd0b-551d-4008-ec1c-b6cb8af384a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /kaggle/working/out\n",
            "creating model and diffusion...\n",
            "sampling...\n",
            "no dpm-solver\n",
            "time for 1 sample 78278.921875\n",
            "no dpm-solver\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-79289bc9a8bf>:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  co = th.tensor(cal_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time for 1 sample 77527.6328125\n",
            "no dpm-solver\n",
            "time for 1 sample 77598.40625\n",
            "no dpm-solver\n",
            "time for 1 sample 79888.0625\n",
            "no dpm-solver\n"
          ]
        }
      ],
      "source": [
        "sample()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/"
      ],
      "metadata": {
        "id": "19YNeVJvfHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhBf4IcFMTDn"
      },
      "outputs": [],
      "source": [
        "!ls /kaggle/working/out\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "display(Image.open('/kaggle/working/out/0010016_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010016_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010016.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010336_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010336_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010336.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010437_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010437_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010437.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010494_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010494_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010494.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010574_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010574_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010574.jpg'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}