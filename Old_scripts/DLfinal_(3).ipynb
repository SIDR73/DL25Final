{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjHp7SysJf-f",
        "outputId": "94a92d2c-c068-4114-a070-c2231756db5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sarlren/isic16?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 832M/832M [00:13<00:00, 66.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/sarlren/isic16/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sarlren/isic16\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IysWIYJdLvzA",
        "outputId": "8f4066fa-a090-4c5e-b296-1291f7933ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedSegDiff'...\n",
            "remote: Enumerating objects: 716, done.\u001b[K\n",
            "remote: Counting objects: 100% (397/397), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 716 (delta 340), reused 248 (delta 248), pack-reused 319 (from 2)\u001b[K\n",
            "Receiving objects: 100% (716/716), 3.90 MiB | 28.56 MiB/s, done.\n",
            "Resolving deltas: 100% (396/396), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WuJunde/MedSegDiff.git\n",
        "!mv MedSegDiff/* ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExdmMQJmKbpQ",
        "outputId": "43fbcd4d-e857-47b9-bc89-375533e365d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir /kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kqDGAVprODFj"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wPgkhLO1ODi0"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fj_l69maOFoc"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle/working/isic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mmB8pgj5cwBe"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/guided_diffusion /content/scripts/guided_diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Qy0qHqODrL",
        "outputId": "0354df5e-267f-4060-de3e-7048c63c405e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import subprocess\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Test_Data/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Training_Data/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Training_GroundTruth/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp -r \"+path+\"/ISBI2016_ISIC_Part1_Test_GroundTruth/* /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp \"+path+\"/ISBI2016_ISIC_Part3B_Test_GroundTruth.csv /kaggle/working/isic/\", shell=True)\n",
        "subprocess.call(\"cp \"+path+\"/ISBI2016_ISIC_Part3B_Training_GroundTruth.csv /kaggle/working/isic/\", shell=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X3KfrcTjJ8VL",
        "outputId": "b093f6ac-4d0a-4c29-e051-461d9adc69b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting blobfile\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile) (2.4.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile) (5.4.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile) (3.18.0)\n",
            "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodomex, blobfile\n",
            "Successfully installed blobfile-3.0.0 pycryptodomex-3.22.0\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.13.2)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m653.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from visdom) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom) (1.15.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from visdom) (3.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from visdom) (11.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2025.4.26)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=34c6a9061bc0eed665a737b7a6aa660f78b44f0b8bf0313ac88bc86c5435138c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built visdom\n",
            "Installing collected packages: visdom\n",
            "Successfully installed visdom-0.2.4\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4458267 sha256=1b7c003d56809ef81e75242d1f61da76c886adf1ca61ab1fd31a0bbb2d946692\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install blobfile\n",
        "!pip install nibabel\n",
        "!pip install visdom\n",
        "!pip install torchsummary\n",
        "!pip install mpi4py\n",
        "!pip install batchgenerators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb"
      ],
      "metadata": {
        "id": "eep4KKwVuOyi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2Z6pr6GhgFOG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/scripts\")\n",
        "sys.path.append(\"./\")\n",
        "from ssl import OP_NO_TLSv1\n",
        "import nibabel as nib\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from PIL import Image\n",
        "import torch.distributed as dist\n",
        "import torchvision.utils as vutils\n",
        "from guided_diffusion import dist_util, logger\n",
        "from guided_diffusion.resample import create_named_schedule_sampler\n",
        "from guided_diffusion.bratsloader import BRATSDataset, BRATSDataset3D\n",
        "from guided_diffusion.isicloader import ISICDataset\n",
        "from guided_diffusion.custom_dataset_loader import CustomDataset\n",
        "from guided_diffusion.script_util import (\n",
        "    NUM_CLASSES,\n",
        "    model_and_diffusion_defaults,\n",
        "    create_model_and_diffusion,\n",
        "    args_to_dict,\n",
        "    add_dict_to_argparser,\n",
        ")\n",
        "import torch as th\n",
        "from guided_diffusion.train_util import TrainLoop\n",
        "from guided_diffusion.utils import staple\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "  data_name: str\n",
        "  data_dir: str\n",
        "  out_dir: str\n",
        "  image_size: int\n",
        "  num_channels: int\n",
        "  class_cond: bool\n",
        "  num_res_blocks: int\n",
        "  num_heads: int\n",
        "  learn_sigma: bool\n",
        "  use_scale_shift_norm: bool\n",
        "  attention_resolutions: str\n",
        "  diffusion_steps: int\n",
        "  noise_schedule: str\n",
        "  rescale_learned_sigmas: bool\n",
        "  rescale_timesteps: bool\n",
        "  lr: float\n",
        "  batch_size: int\n",
        "  patience: int\n",
        "  min_delta: float\n",
        "  microbatch: int\n",
        "  ema_rate: str\n",
        "  log_interval: int\n",
        "  save_interval: int\n",
        "  resume_checkpoint: str\n",
        "  schedule_sampler: str\n",
        "  weight_decay: float\n",
        "  lr_anneal_steps: int\n",
        "  use_fp16: bool\n",
        "  fp16_scale_growth: float\n",
        "  gpu_dev: str\n",
        "  multi_gpu: str\n",
        "  in_ch: int\n",
        "  num_heads_upsample: int\n",
        "  num_head_channels: int\n",
        "  resblock_updown: bool\n",
        "  dpm_solver: bool\n",
        "  version: str\n",
        "  channel_mult: str\n",
        "  dropout: float\n",
        "  use_checkpoint: bool\n",
        "  use_new_attention_order: bool\n",
        "  timestep_respacing: str\n",
        "  use_kl: bool\n",
        "  predict_xstart: bool\n",
        "  model_path: str\n",
        "  num_ensemble: int\n",
        "  use_ddim: bool\n",
        "  clip_denoised: bool\n",
        "\n",
        "argsdict = dict(\n",
        "  data_name = 'ISIC',\n",
        "  data_dir = \"/kaggle/working/isic\",\n",
        "  out_dir = \"/kaggle/working/out\",\n",
        "  image_size = 256,\n",
        "  num_channels = 128,\n",
        "  class_cond = False,\n",
        "  num_res_blocks = 2,\n",
        "  num_heads = 1,\n",
        "  learn_sigma = True,\n",
        "  use_scale_shift_norm = False,\n",
        "  attention_resolutions = \"16\",\n",
        "  diffusion_steps = 1000,\n",
        "  noise_schedule = 'linear',\n",
        "  rescale_learned_sigmas = False,\n",
        "  rescale_timesteps = False,\n",
        "  lr = 1e-4,\n",
        "  batch_size = 4,\n",
        "  patience = 2,\n",
        "  min_delta = 0.001,\n",
        "  microbatch = -1,  # -1 disables microbatches\n",
        "  ema_rate = \"0.9999\",  # comma-separated list of EMA values\n",
        "  log_interval = 100,\n",
        "  save_interval = 5000,\n",
        "  resume_checkpoint = None, #\"/results/pretrainedmodel.pt\"\n",
        "  schedule_sampler=\"uniform\",\n",
        "  weight_decay=0.0,\n",
        "  lr_anneal_steps=400, # used for early stopping\n",
        "  use_fp16=False,\n",
        "  fp16_scale_growth=1e-3,\n",
        "  gpu_dev = \"0\",\n",
        "  multi_gpu = None, #\"0,1,2\"\n",
        "\n",
        "  # useful for sampling only:\n",
        "  model_path = \"/kaggle/working/out/savedmodel000400.pt\",\n",
        "  num_ensemble = 5,\n",
        "  use_ddim = False, #originally False\n",
        "  clip_denoised = True,\n",
        ")\n",
        "argsdict.update(model_and_diffusion_defaults())\n",
        "\n",
        "args = Args(**argsdict)\n",
        "\n",
        "seed=10\n",
        "th.manual_seed(seed)\n",
        "th.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    dist_util.setup_dist(args)\n",
        "    logger.configure(dir = args.out_dir)\n",
        "\n",
        "    logger.log(\"creating data loader...\")\n",
        "\n",
        "    if args.data_name == 'ISIC':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = ISICDataset(args, args.data_dir, transform_train)\n",
        "        args.in_ch = 4\n",
        "    elif args.data_name == 'BRATS':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = BRATSDataset3D(args.data_dir, transform_train, test_flag=False)\n",
        "        args.in_ch = 5\n",
        "    else :\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_train = transforms.Compose(tran_list)\n",
        "        print(\"Your current directory : \",args.data_dir)\n",
        "        ds = CustomDataset(args, args.data_dir, transform_train)\n",
        "        args.in_ch = 4\n",
        "\n",
        "    datal= th.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True)\n",
        "    data = iter(datal)\n",
        "\n",
        "    logger.log(\"creating model and diffusion...\")\n",
        "\n",
        "    model, diffusion = create_model_and_diffusion(\n",
        "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
        "    )\n",
        "    if args.multi_gpu:\n",
        "        model = th.nn.DataParallel(model,device_ids=[int(id) for id in args.multi_gpu.split(',')])\n",
        "        model.to(device = th.device('cuda', int(args.gpu_dev)))\n",
        "    else:\n",
        "        model.to(dist_util.dev())\n",
        "    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion,  maxt=args.diffusion_steps)\n",
        "\n",
        "\n",
        "    logger.log(\"training...\")\n",
        "    TrainLoop(\n",
        "        model=model,\n",
        "        diffusion=diffusion,\n",
        "        classifier=None,\n",
        "        data=data,\n",
        "        dataloader=datal,\n",
        "        batch_size=args.batch_size,\n",
        "        microbatch=args.microbatch,\n",
        "        lr=args.lr,\n",
        "        ema_rate=args.ema_rate,\n",
        "        log_interval=args.log_interval,\n",
        "        save_interval=args.save_interval,\n",
        "        resume_checkpoint=args.resume_checkpoint,\n",
        "        use_fp16=args.use_fp16,\n",
        "        fp16_scale_growth=args.fp16_scale_growth,\n",
        "        schedule_sampler=schedule_sampler,\n",
        "        weight_decay=args.weight_decay,\n",
        "        lr_anneal_steps=args.lr_anneal_steps,\n",
        "    ).run_loop()\n",
        "\n",
        "def visualize(img):\n",
        "    _min = img.min()\n",
        "    _max = img.max()\n",
        "    normalized_img = (img - _min)/ (_max - _min)\n",
        "    return normalized_img\n",
        "\n",
        "\n",
        "def sample():\n",
        "    dist_util.setup_dist(args)\n",
        "    logger.configure(dir = args.out_dir)\n",
        "\n",
        "    if args.data_name == 'ISIC':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor(),]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = ISICDataset(args, args.data_dir, transform_test, mode = 'Test')\n",
        "        args.in_ch = 4\n",
        "    elif args.data_name == 'BRATS':\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)),]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = BRATSDataset3D(args.data_dir,transform_test)\n",
        "        args.in_ch = 5\n",
        "    else:\n",
        "        tran_list = [transforms.Resize((args.image_size,args.image_size)), transforms.ToTensor()]\n",
        "        transform_test = transforms.Compose(tran_list)\n",
        "\n",
        "        ds = CustomDataset(args, args.data_dir, transform_test, mode = 'Test')\n",
        "        args.in_ch = 4\n",
        "\n",
        "    datal = th.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True)\n",
        "    data = iter(datal)\n",
        "\n",
        "    logger.log(\"creating model and diffusion...\")\n",
        "\n",
        "    model, diffusion = create_model_and_diffusion(\n",
        "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
        "    )\n",
        "    all_images = []\n",
        "\n",
        "\n",
        "    state_dict = dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n",
        "    from collections import OrderedDict\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        # name = k[7:] # remove `module.`\n",
        "        if 'module.' in k:\n",
        "            new_state_dict[k[7:]] = v\n",
        "            # load params\n",
        "        else:\n",
        "            new_state_dict = state_dict\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "    model.to(dist_util.dev())\n",
        "    if args.use_fp16:\n",
        "        model.convert_to_fp16()\n",
        "    model.eval()\n",
        "    for _ in range(len(data)):\n",
        "        b, m, path = next(data)  #should return an image from the dataloader \"data\"\n",
        "        c = th.randn_like(b[:, :1, ...])\n",
        "        img = th.cat((b, c), dim=1)     #add a noise channel$\n",
        "        if args.data_name == 'ISIC':\n",
        "            slice_ID=path[0].split(\"_\")[-1].split('.')[0]\n",
        "        elif args.data_name == 'BRATS':\n",
        "            # slice_ID=path[0].split(\"_\")[2] + \"_\" + path[0].split(\"_\")[4]\n",
        "            slice_ID=path[0].split(\"_\")[-3] + \"_\" + path[0].split(\"slice\")[-1].split('.nii')[0]\n",
        "\n",
        "        logger.log(\"sampling...\")\n",
        "\n",
        "        start = th.cuda.Event(enable_timing=True)\n",
        "        end = th.cuda.Event(enable_timing=True)\n",
        "        enslist = []\n",
        "\n",
        "        for i in range(args.num_ensemble):  #this is for the generation of an ensemble of 5 masks.\n",
        "            model_kwargs = {}\n",
        "            start.record()\n",
        "            sample_fn = (\n",
        "                diffusion.p_sample_loop_known if not args.use_ddim else diffusion.ddim_sample_loop_known\n",
        "            )\n",
        "            # pdb.set_trace() #imbedded debug !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "            # truncating model output after its internally calculated in dim 2 should fix the issue:\n",
        "            # https://github.com/SuperMedIntel/MedSegDiff/issues/201\n",
        "            # fixable by just editing gaussian_diffusion.py:346 to be permissive\n",
        "            sample, x_noisy, org, cal, cal_out = sample_fn( # broken\n",
        "                model,\n",
        "                (args.batch_size, 3, args.image_size, args.image_size), img,\n",
        "                step = args.diffusion_steps,\n",
        "                clip_denoised=args.clip_denoised,\n",
        "                model_kwargs=model_kwargs,\n",
        "            )\n",
        "\n",
        "            end.record()\n",
        "            th.cuda.synchronize()\n",
        "            print('time for 1 sample', start.elapsed_time(end))  #time measurement for the generation of 1 sample\n",
        "\n",
        "            co = th.tensor(cal_out)\n",
        "            if args.version == 'new':\n",
        "                enslist.append(sample[:,-1,:,:])\n",
        "            else:\n",
        "                enslist.append(co)\n",
        "\n",
        "            if args.debug:\n",
        "                # print('sample size is',sample.size())\n",
        "                # print('org size is',org.size())\n",
        "                # print('cal size is',cal.size())\n",
        "                if args.data_name == 'ISIC':\n",
        "                    # s = th.tensor(sample)[:,-1,:,:].unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "                    o = th.tensor(org)[:,:-1,:,:]\n",
        "                    c = th.tensor(cal).repeat(1, 3, 1, 1)\n",
        "                    # co = co.repeat(1, 3, 1, 1)\n",
        "\n",
        "                    s = sample[:,-1,:,:]\n",
        "                    b,h,w = s.size()\n",
        "                    ss = s.clone()\n",
        "                    ss = ss.view(s.size(0), -1)\n",
        "                    ss -= ss.min(1, keepdim=True)[0]\n",
        "                    ss /= ss.max(1, keepdim=True)[0]\n",
        "                    ss = ss.view(b, h, w)\n",
        "                    ss = ss.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "\n",
        "                    tup = (ss,o,c)\n",
        "                elif args.data_name == 'BRATS':\n",
        "                    s = th.tensor(sample)[:,-1,:,:].unsqueeze(1)\n",
        "                    m = th.tensor(m.to(device = 'cuda:0'))[:,0,:,:].unsqueeze(1)\n",
        "                    o1 = th.tensor(org)[:,0,:,:].unsqueeze(1)\n",
        "                    o2 = th.tensor(org)[:,1,:,:].unsqueeze(1)\n",
        "                    o3 = th.tensor(org)[:,2,:,:].unsqueeze(1)\n",
        "                    o4 = th.tensor(org)[:,3,:,:].unsqueeze(1)\n",
        "                    c = th.tensor(cal)\n",
        "\n",
        "                    tup = (o1/o1.max(),o2/o2.max(),o3/o3.max(),o4/o4.max(),m,s,c,co)\n",
        "\n",
        "                compose = th.cat(tup,0)\n",
        "                vutils.save_image(compose, fp = os.path.join(args.out_dir, str(slice_ID)+'_output'+str(i)+\".jpg\"), nrow = 1, padding = 10)\n",
        "        ensres = staple(th.stack(enslist,dim=0)).squeeze(0)\n",
        "        vutils.save_image(ensres, fp = os.path.join(args.out_dir, str(slice_ID)+'_output_ens'+\".jpg\"), nrow = 1, padding = 10)"
      ],
      "metadata": {
        "id": "9wuvHH-DiCRk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGQ1YgmHi3Vx",
        "outputId": "131581ae-2f6f-4132-f484-5265c07133a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /kaggle/working/out\n",
            "creating data loader...\n",
            "creating model and diffusion...\n",
            "training...\n",
            "---------------------------\n",
            "| grad_norm    | 21.9     |\n",
            "| loss         | 1        |\n",
            "| loss_cal     | 0.386    |\n",
            "| loss_cal_q0  | 0.482    |\n",
            "| loss_cal_q2  | 0.374    |\n",
            "| loss_cal_q3  | 0.315    |\n",
            "| loss_diff    | 1        |\n",
            "| loss_diff_q0 | 1.01     |\n",
            "| loss_diff_q2 | 0.995    |\n",
            "| loss_diff_q3 | 1.01     |\n",
            "| loss_q0      | 1.01     |\n",
            "| loss_q2      | 0.995    |\n",
            "| loss_q3      | 1.01     |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 4        |\n",
            "| step         | 0        |\n",
            "---------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n",
            "---------------------------\n",
            "| grad_norm    | 16.1     |\n",
            "| loss         | 0.258    |\n",
            "| loss_cal     | 0.237    |\n",
            "| loss_cal_q0  | 0.221    |\n",
            "| loss_cal_q1  | 0.242    |\n",
            "| loss_cal_q2  | 0.245    |\n",
            "| loss_cal_q3  | 0.238    |\n",
            "| loss_diff    | 0.258    |\n",
            "| loss_diff_q0 | 0.334    |\n",
            "| loss_diff_q1 | 0.256    |\n",
            "| loss_diff_q2 | 0.228    |\n",
            "| loss_diff_q3 | 0.209    |\n",
            "| loss_q0      | 0.334    |\n",
            "| loss_q1      | 0.256    |\n",
            "| loss_q2      | 0.228    |\n",
            "| loss_q3      | 0.209    |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 404      |\n",
            "| step         | 100      |\n",
            "---------------------------\n",
            "---------------------------\n",
            "| grad_norm    | 10.6     |\n",
            "| loss         | 0.0467   |\n",
            "| loss_cal     | 0.159    |\n",
            "| loss_cal_q0  | 0.164    |\n",
            "| loss_cal_q1  | 0.151    |\n",
            "| loss_cal_q2  | 0.154    |\n",
            "| loss_cal_q3  | 0.165    |\n",
            "| loss_diff    | 0.0467   |\n",
            "| loss_diff_q0 | 0.125    |\n",
            "| loss_diff_q1 | 0.0294   |\n",
            "| loss_diff_q2 | 0.0166   |\n",
            "| loss_diff_q3 | 0.0157   |\n",
            "| loss_q0      | 0.125    |\n",
            "| loss_q1      | 0.0294   |\n",
            "| loss_q2      | 0.0166   |\n",
            "| loss_q3      | 0.0157   |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 804      |\n",
            "| step         | 200      |\n",
            "---------------------------\n",
            "---------------------------\n",
            "| grad_norm    | 9.87     |\n",
            "| loss         | 0.0277   |\n",
            "| loss_cal     | 0.15     |\n",
            "| loss_cal_q0  | 0.157    |\n",
            "| loss_cal_q1  | 0.151    |\n",
            "| loss_cal_q2  | 0.141    |\n",
            "| loss_cal_q3  | 0.148    |\n",
            "| loss_diff    | 0.0277   |\n",
            "| loss_diff_q0 | 0.0716   |\n",
            "| loss_diff_q1 | 0.017    |\n",
            "| loss_diff_q2 | 0.00757  |\n",
            "| loss_diff_q3 | 0.00664  |\n",
            "| loss_q0      | 0.0716   |\n",
            "| loss_q1      | 0.017    |\n",
            "| loss_q2      | 0.00757  |\n",
            "| loss_q3      | 0.00664  |\n",
            "| param_norm   | 227      |\n",
            "| samples      | 1.2e+03  |\n",
            "| step         | 300      |\n",
            "---------------------------\n",
            "saving model 0...\n",
            "saving model 0.9999...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blvwy1lcgFd3"
      },
      "source": [
        "OLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5lkM8JvL9fM",
        "outputId": "8f502541-946f-4205-83c7-fcadceb96ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emasavedmodel_0.9999_000000.pt\toptsavedmodel000000.pt\tsavedmodel000000.pt\n",
            "emasavedmodel_0.9999_000004.pt\toptsavedmodel000004.pt\tsavedmodel000004.pt\n",
            "emasavedmodel_0.9999_000040.pt\toptsavedmodel000040.pt\tsavedmodel000040.pt\n",
            "emasavedmodel_0.9999_000400.pt\toptsavedmodel000400.pt\tsavedmodel000400.pt\n",
            "log.txt\t\t\t\tprogress.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "udrPkrJqL9_k",
        "outputId": "8d54307d-d5e0-468d-9bc9-ee155a65116c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/working/out\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/kaggle/working/out/optsavedmodel000000.pt"
            ],
            "text/html": [
              "<a href='/kaggle/working/out/optsavedmodel000000.pt' target='_blank'>/kaggle/working/out/optsavedmodel000000.pt</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "%cd /kaggle/working/out\n",
        "from IPython.display import FileLink\n",
        "FileLink('/kaggle/working/out/optsavedmodel000000.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_3tkd7lMAMR",
        "outputId": "791ea8ea-226e-42a7-8e78-60df4aa0801f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "tar: /kaggle/working/out/models.tar: file is the archive; not dumped\n"
          ]
        }
      ],
      "source": [
        "!tar -cf models.tar /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MLMGVTlMMBrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28d1349-46e8-4daa-d155-61b3345d560f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emasavedmodel_0.9999_000000.pt\tmodels.tar\t\tprogress.csv\n",
            "emasavedmodel_0.9999_000004.pt\toptsavedmodel000000.pt\tsavedmodel000000.pt\n",
            "emasavedmodel_0.9999_000040.pt\toptsavedmodel000004.pt\tsavedmodel000004.pt\n",
            "emasavedmodel_0.9999_000400.pt\toptsavedmodel000040.pt\tsavedmodel000040.pt\n",
            "log.txt\t\t\t\toptsavedmodel000400.pt\tsavedmodel000400.pt\n"
          ]
        }
      ],
      "source": [
        "!ls /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7pHdtluDMC5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40f867c-3b75-4356-c020-c0501bfd8c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/kaggle/working/out/\n",
            "/kaggle/working/out/savedmodel000400.pt\n",
            "/kaggle/working/out/savedmodel000000.pt\n",
            "/kaggle/working/out/savedmodel000040.pt\n",
            "/kaggle/working/out/optsavedmodel000004.pt\n",
            "/kaggle/working/out/optsavedmodel000400.pt\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000000.pt\n",
            "/kaggle/working/out/optsavedmodel000040.pt\n",
            "/kaggle/working/out/model_tar2.tar.gz\n",
            "tar: /kaggle/working/out/model_tar2.tar.gz: file changed as we read it\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000040.pt\n",
            "/kaggle/working/out/optsavedmodel000000.pt\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000004.pt\n",
            "/kaggle/working/out/progress.csv\n",
            "/kaggle/working/out/emasavedmodel_0.9999_000400.pt\n",
            "/kaggle/working/out/savedmodel000004.pt\n",
            "/kaggle/working/out/log.txt\n",
            "/kaggle/working/out/models.tar\n",
            "tar: /kaggle/working/out: file changed as we read it\n"
          ]
        }
      ],
      "source": [
        "!tar czvf model_tar2.tar.gz /kaggle/working/out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1NgQslhBMElC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39246eb9-f3a4-4eaf-f51d-a4c95b1546d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 27G\n",
            "drwxr-xr-x 2 root root 4.0K May  3 05:23 .\n",
            "drwxr-xr-x 4 root root 4.0K May  3 05:09 ..\n",
            "-rw-r--r-- 1 root root 412M May  3 05:18 emasavedmodel_0.9999_000000.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:16 emasavedmodel_0.9999_000004.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:18 emasavedmodel_0.9999_000040.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:21 emasavedmodel_0.9999_000400.pt\n",
            "-rw-r--r-- 1 root root 2.4K May  3 05:21 log.txt\n",
            "-rw-r--r-- 1 root root 6.5G May  3 05:23 models.tar\n",
            "-rw-r--r-- 1 root root  14G May  3 05:37 model_tar2.tar.gz\n",
            "-rw-r--r-- 1 root root 823M May  3 05:18 optsavedmodel000000.pt\n",
            "-rw-r--r-- 1 root root 823M May  3 05:16 optsavedmodel000004.pt\n",
            "-rw-r--r-- 1 root root 823M May  3 05:18 optsavedmodel000040.pt\n",
            "-rw-r--r-- 1 root root 823M May  3 05:21 optsavedmodel000400.pt\n",
            "-rw-r--r-- 1 root root 1.1K May  3 05:21 progress.csv\n",
            "-rw-r--r-- 1 root root 412M May  3 05:18 savedmodel000000.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:16 savedmodel000004.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:18 savedmodel000040.pt\n",
            "-rw-r--r-- 1 root root 412M May  3 05:21 savedmodel000400.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -alh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "b6h7nl_dMF3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa752c9-94f6-43f7-a884-26648ba526d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drwxr-xr-x root/root         0 2025-05-03 05:22 kaggle/working/out/\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 05:21 kaggle/working/out/savedmodel000400.pt\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 05:18 kaggle/working/out/savedmodel000000.pt\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 05:18 kaggle/working/out/savedmodel000040.pt\n",
            "-rw-r--r-- root/root 862379209 2025-05-03 05:16 kaggle/working/out/optsavedmodel000004.pt\n",
            "-rw-r--r-- root/root 862379209 2025-05-03 05:21 kaggle/working/out/optsavedmodel000400.pt\n",
            "-rw-r--r-- root/root 431251992 2025-05-03 05:18 kaggle/working/out/emasavedmodel_0.9999_000000.pt\n",
            "-rw-r--r-- root/root 862379209 2025-05-03 05:18 kaggle/working/out/optsavedmodel000040.pt\n",
            "-rw-r--r-- root/root 3764387840 2025-05-03 05:27 kaggle/working/out/model_tar2.tar.gz\n",
            "-rw-r--r-- root/root  431251992 2025-05-03 05:18 kaggle/working/out/emasavedmodel_0.9999_000040.pt\n",
            "-rw-r--r-- root/root  862379209 2025-05-03 05:18 kaggle/working/out/optsavedmodel000000.pt\n",
            "-rw-r--r-- root/root  431251992 2025-05-03 05:16 kaggle/working/out/emasavedmodel_0.9999_000004.pt\n",
            "-rw-r--r-- root/root       1102 2025-05-03 05:21 kaggle/working/out/progress.csv\n",
            "-rw-r--r-- root/root  431251992 2025-05-03 05:21 kaggle/working/out/emasavedmodel_0.9999_000400.pt\n",
            "-rw-r--r-- root/root  431251992 2025-05-03 05:16 kaggle/working/out/savedmodel000004.pt\n",
            "-rw-r--r-- root/root       2449 2025-05-03 05:21 kaggle/working/out/log.txt\n",
            "-rw-r--r-- root/root 6899558400 2025-05-03 05:23 kaggle/working/out/models.tar\n"
          ]
        }
      ],
      "source": [
        "!tar -tvf ./model_tar2.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f65s16gQguvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8542b9a6-79ab-4fe9-cb36-c574b20b5ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia_smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia_smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OZDlerMDMQP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "4abcbd0b-f49a-4cee-a776-ac9f2b195e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to /kaggle/working/out\n",
            "creating model and diffusion...\n",
            "sampling...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "only integer tensors of a single element can be converted to an index",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6f614e1d4e6c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ae11ddd88bb8>\u001b[0m in \u001b[0;36msample\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m             )\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m#pdb.set_trace() #imbedded debug !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             sample, x_noisy, org, cal, cal_out = sample_fn(\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/guided_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mddim_sample_loop_known\u001b[0;34m(self, model, shape, img, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress, eta)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         for sample in self.ddim_sample_loop_progressive(\n\u001b[0m\u001b[1;32m    868\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/guided_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mddim_sample_loop_progressive\u001b[0;34m(self, model, shape, time, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress, eta)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0morghigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ],
      "source": [
        "sample()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/"
      ],
      "metadata": {
        "id": "19YNeVJvfHUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhBf4IcFMTDn"
      },
      "outputs": [],
      "source": [
        "!ls /kaggle/working/out\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "display(Image.open('/kaggle/working/out/0010016_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010016_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010016.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010336_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010336_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010336.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010437_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010437_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010437.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010494_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010494_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010494.jpg'))\n",
        "\n",
        "display(Image.open('/kaggle/working/out/0010574_output_ens.jpg'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010574_Segmentation.png'))\n",
        "display(Image.open('/kaggle/working/isic/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010574.jpg'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}